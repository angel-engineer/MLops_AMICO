{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c8d50e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de librerias\n",
    "# ------------------\n",
    "# Librerias de uso general\n",
    "import holidays\n",
    "from google.cloud import storage\n",
    "\n",
    "# Manejo de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM  # Importación para OC-SVM\n",
    "from sklearn.neighbors import LocalOutlierFactor # Importación para LOF\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Estadística y series temporales\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Se importan las funciones\n",
    "from sklearn.metrics import  mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a1a5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 0. configuración ----------\n",
    "CSV_PATH = \"/Users/angeleduardogamarrarios/Repositorio_UDEM/MLops_AMICO/data/costs.csv\"       # ajusta si hace falta\n",
    "TEST_DAYS = 60               # últimos N días para test\n",
    "RND_ITER = 12                # RandomizedSearchCV iteraciones (ajusta)\n",
    "TS_SPLITS = 3                # TimeSeriesSplit folds (ajusta)\n",
    "PERM_REPEATS = 10            # permutación (ajusta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2545613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 351 Columnas: ['Relational Database Service($)', 'EC2-Instances($)', 'FSx($)', 'Elastic File System($)', 'EC2-Other($)', 'CloudWatch($)', 'S3($)', 'Elastic Load Balancing($)', 'Backup($)', 'Key Management Service($)', 'DataSync($)', 'Secrets Manager($)', 'Resilience Hub($)', 'Total costs($)']\n"
     ]
    }
   ],
   "source": [
    "# ---------- 1. carga y limpieza básica ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# eliminar fila de \"Service total\" si existe\n",
    "df = df[df['Service'] != 'Service total']\n",
    "\n",
    "# convertir columna de fecha en indice\n",
    "df['Service'] = pd.to_datetime(df['Service'])\n",
    "df = df.rename(columns={'Service':'date'}).sort_values('date').set_index('date')\n",
    "\n",
    "# forzar numérico y revisar columnas\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "num_cols = df.columns.tolist()\n",
    "\n",
    "print(\"Filas:\", len(df), \"Columnas:\", num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b624c140",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapper = {\n",
    "    'Service': 'fecha',\n",
    "    'Relational Database Service($)': 'rds',\n",
    "    'EC2-Instances($)': 'ec2',\n",
    "    'FSx($)': 'fsx',\n",
    "    'Elastic File System($)': 'efs',\n",
    "    'EC2-Other($)': 'ec2_other',\n",
    "    'CloudWatch($)': 'cloudwatch',\n",
    "    'Elastic Load Balancing($)': 'elb',\n",
    "    'S3($)': 's3',\n",
    "    'Backup($)': 'backup',\n",
    "    'Key Management Service($)': 'kms',\n",
    "    'DataSync($)': 'data_sync',\n",
    "    'Secrets Manager($)': 'secrets_manager',\n",
    "    'Resilience Hub($)': 'resiliency',\n",
    "    'Total costs($)': 'total_costs'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0970734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo de columnas \n",
    "df.rename(columns=column_mapper, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f22d9e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen estadístico:\n",
      "                 count        mean         std           min         25%  \\\n",
      "rds              351.0   97.240692   87.975006  1.491792e+01   41.109960   \n",
      "ec2              329.0   71.525192   25.109641  5.154986e-01   60.758766   \n",
      "fsx              351.0   14.678272    1.855596  1.208052e+01   12.584705   \n",
      "efs              351.0    8.064240    6.456387  1.786584e+00    2.040983   \n",
      "ec2_other        351.0    6.076750    2.900253  3.117944e-01    5.541606   \n",
      "cloudwatch       351.0    4.398582    7.681060  3.109096e-03    0.478766   \n",
      "s3               351.0    2.218248    0.787167  1.023371e+00    1.693569   \n",
      "elb              351.0    2.162289    0.004710  2.160000e+00    2.160033   \n",
      "backup           350.0    1.514773    1.210465  2.810710e-01    0.322099   \n",
      "kms              351.0    0.260617    0.032317  2.263914e-01    0.237089   \n",
      "data_sync        119.0    0.286251    1.415347  6.440000e-08    0.000001   \n",
      "secrets_manager  349.0    0.000513    0.000527  2.000000e-05    0.000245   \n",
      "resiliency         3.0    0.000000    0.000000  0.000000e+00    0.000000   \n",
      "total_costs      351.0  203.749835  123.021125  3.366081e+01  127.055868   \n",
      "\n",
      "                        50%         75%         max  \n",
      "rds               66.762613  118.507451  455.461320  \n",
      "ec2               68.402379   95.067461  100.737059  \n",
      "fsx               15.468255   16.056011   18.491514  \n",
      "efs                4.627442   14.108374   20.584791  \n",
      "ec2_other          6.124120    7.267743   11.823171  \n",
      "cloudwatch         0.682977    1.661212   25.383237  \n",
      "s3                 2.084439    3.082417    3.540238  \n",
      "elb                2.160623    2.162270    2.194784  \n",
      "backup             1.043619    2.500407    4.093945  \n",
      "kms                0.258728    0.267448    0.356186  \n",
      "data_sync          0.000001    0.001859    8.782443  \n",
      "secrets_manager    0.000455    0.000640    0.006030  \n",
      "resiliency         0.000000    0.000000    0.000000  \n",
      "total_costs      169.308792  262.386479  606.597727  \n",
      "\n",
      "Skewness (top):\n",
      "secrets_manager    6.825494\n",
      "data_sync          5.243936\n",
      "elb                3.825244\n",
      "rds                1.866276\n",
      "cloudwatch         1.677079\n",
      "kms                1.537350\n",
      "total_costs        0.975280\n",
      "efs                0.664392\n",
      "backup             0.622538\n",
      "s3                 0.119986\n",
      "dtype: float64\n",
      "\n",
      "Media por día de la semana (muestra):\n",
      "                 rds        ec2        fsx       efs  ec2_other  cloudwatch  \\\n",
      "date                                                                          \n",
      "Friday    104.490301  76.534417  14.801883  8.218541   7.052756    4.532564   \n",
      "Monday     97.532270  75.251444  14.722385  7.949294   6.826152    4.525775   \n",
      "Saturday   90.899097  44.656602  14.535091  8.179670   4.083944    3.875943   \n",
      "Sunday     83.598953  75.433716  14.472918  7.941768   3.811028    3.811151   \n",
      "Thursday  103.113998  76.944815  14.770521  8.026860   6.897890    4.715092   \n",
      "\n",
      "                s3       elb    backup       kms  data_sync  secrets_manager  \\\n",
      "date                                                                           \n",
      "Friday    2.239331  2.163086  1.543027  0.261761   1.264603         0.000579   \n",
      "Monday    2.209111  2.162876  1.479395  0.260807   0.073895         0.000552   \n",
      "Saturday  2.207464  2.161187  1.527428  0.260784   0.053987         0.000396   \n",
      "Sunday    2.189945  2.160560  1.518090  0.258952   0.000705         0.000420   \n",
      "Thursday  2.233260  2.163299  1.514082  0.260705   0.001682         0.000621   \n",
      "\n",
      "          resiliency  total_costs  \n",
      "date                               \n",
      "Friday           NaN   222.318796  \n",
      "Monday           0.0   212.945186  \n",
      "Saturday         0.0   172.377762  \n",
      "Sunday           NaN   162.006882  \n",
      "Thursday         0.0   220.641680  \n"
     ]
    }
   ],
   "source": [
    "# ---------- 2. EDA rápido (resumen + skew + patrón semanal) ----------\n",
    "print(\"\\nResumen estadístico:\")\n",
    "print(df.describe().T)\n",
    "\n",
    "skewness = df.skew().sort_values(ascending=False)\n",
    "print(\"\\nSkewness (top):\")\n",
    "print(skewness.head(10))\n",
    "\n",
    "weekly_mean = df.groupby(df.index.day_name()).mean()\n",
    "print(\"\\nMedia por día de la semana (muestra):\")\n",
    "print(weekly_mean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97db7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3. imputación ----------\n",
    "# Strategy: cambiar a 0\n",
    "df_imputed = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80fcfa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box-Cox aplicado a Secrets Manager($), lambda=0.3130\n",
      "Box-Cox aplicado a DataSync($), lambda=-0.7391\n",
      "Box-Cox aplicado a Elastic Load Balancing($), lambda=-693.3143\n",
      "Box-Cox aplicado a Relational Database Service($), lambda=-0.0614\n",
      "Box-Cox aplicado a CloudWatch($), lambda=0.0712\n",
      "Box-Cox aplicado a Key Management Service($), lambda=-4.2383\n"
     ]
    }
   ],
   "source": [
    "# ---------- 4. Box-Cox selectivo ----------\n",
    "# Aplicar Box-Cox solo a columnas muy sesgadas (skew > 1)\n",
    "skewed_cols = skewness[skewness > 1].index.tolist()\n",
    "df_bc = df_imputed.copy()\n",
    "\n",
    "for c in skewed_cols:\n",
    "    # Box-Cox exige valores > 0. si hay ceros o negativos shift pequeño\n",
    "    min_val = df_bc[c].min()\n",
    "    shift = 0.0 if min_val > 0 else abs(min_val) + 1e-6\n",
    "    try:\n",
    "        transformed, lam = boxcox(df_bc[c] + shift)\n",
    "        df_bc[c] = transformed\n",
    "        print(f\"Box-Cox aplicado a {c}, lambda={lam:.4f}\")\n",
    "    except Exception as e:\n",
    "        # fallback log1p si falla\n",
    "        df_bc[c] = np.log1p(df_bc[c] + shift)\n",
    "        print(f\"Box-Cox falló en {c}, aplicado log1p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3da2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5. normalización por día de la semana ----------\n",
    "df_bc['day_of_week'] = df_bc.index.day_name()\n",
    "scaled = df_bc.copy()\n",
    "features = [c for c in num_cols]  # lista de features reales\n",
    "\n",
    "for day in scaled['day_of_week'].unique():\n",
    "    mask = scaled['day_of_week'] == day\n",
    "    if mask.sum() < 2:\n",
    "        # si no hay suficientes ejemplos para el día, omitir\n",
    "        continue\n",
    "    scaler = StandardScaler()\n",
    "    scaled.loc[mask, features] = scaler.fit_transform(scaled.loc[mask, features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a04092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (290, 14), Test shape: (61, 14)\n"
     ]
    }
   ],
   "source": [
    "# ---------- 6. división train/test temporal ----------\n",
    "split_date = scaled.index.max() - pd.Timedelta(days=TEST_DAYS)\n",
    "train_df = scaled[scaled.index < split_date].drop(columns=['day_of_week'])\n",
    "test_df = scaled[scaled.index >= split_date].drop(columns=['day_of_week'])\n",
    "\n",
    "X_train = train_df.values\n",
    "X_test = test_df.values\n",
    "cols = train_df.columns.tolist()\n",
    "\n",
    "print(f\"\\nTrain shape: {X_train.shape}, Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b53947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "\n",
      "Mejores parámetros (RandomizedSearch):\n",
      "{'n_estimators': 50, 'max_samples': 0.9, 'max_features': 0.7, 'contamination': 0.001}\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "\n",
      "Mejores parámetros (GridSearch):\n",
      "{'contamination': 0.0005, 'max_features': 0.7, 'max_samples': 0.9, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# ---------- 7. búsqueda de hiperparámetros ----------\n",
    "# scoring personalizado: media de decision_function (cuanto mayor, mejor)\n",
    "def scoring_fn(estimator, X, y=None):\n",
    "    return float(np.mean(estimator.decision_function(X)))\n",
    "\n",
    "iso = IsolationForest(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_samples': [0.5, 0.7, 0.9, 'auto'],\n",
    "    'contamination': [0.001, 0.005, 0.01, 0.02, 0.05],\n",
    "    'max_features': [0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=TS_SPLITS)\n",
    "rnd = RandomizedSearchCV(iso, param_distributions=param_dist, n_iter=RND_ITER,\n",
    "                         cv=tscv, random_state=42, n_jobs=-1, scoring=scoring_fn, verbose=1)\n",
    "rnd.fit(X_train)\n",
    "\n",
    "print(\"\\nMejores parámetros (RandomizedSearch):\")\n",
    "print(rnd.best_params_)\n",
    "\n",
    "# ajustar pequeño grid alrededor del mejor para refinar (GridSearch)\n",
    "best = rnd.best_params_\n",
    "grid = {\n",
    "    'n_estimators': sorted(list({max(10, best['n_estimators']-50), best['n_estimators'], best['n_estimators']+50})),\n",
    "    'max_samples': sorted(list(set([best['max_samples'] if best['max_samples']=='auto' else max(0.1, best['max_samples']-0.1), best['max_samples'], min(1.0, best['max_samples']+0.1)]))),\n",
    "    'contamination': sorted(list({max(0.0005, best['contamination']/2), best['contamination'], min(0.1, best['contamination']*2)})),\n",
    "    'max_features': sorted(list({max(0.1, best['max_features']-0.2), best['max_features'], min(1.0, best['max_features']+0.2)}))\n",
    "}\n",
    "# limpiar valores inválidos\n",
    "grid['max_samples'] = [v for v in grid['max_samples'] if (isinstance(v, str) or (isinstance(v, float) and 0 < v <= 1))]\n",
    "\n",
    "gsearch = GridSearchCV(IsolationForest(random_state=42), param_grid=grid, cv=tscv, n_jobs=-1, scoring=scoring_fn, verbose=1)\n",
    "gsearch.fit(X_train)\n",
    "\n",
    "print(\"\\nMejores parámetros (GridSearch):\")\n",
    "print(gsearch.best_params_)\n",
    "\n",
    "best_model = gsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ce20e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anomalías en test (conteo): 27\n",
      "            anomaly_score  anomaly\n",
      "date                              \n",
      "2025-03-18       0.112032        0\n",
      "2025-03-19       0.124936        0\n",
      "2025-03-20       0.112604        0\n",
      "2025-03-21       0.112991        0\n",
      "2025-03-22       0.107334        0\n",
      "2025-03-23       0.075393        0\n",
      "2025-03-24       0.070000        0\n",
      "2025-03-25       0.049060        0\n",
      "2025-03-26       0.062226        0\n",
      "2025-03-27       0.062226        0\n",
      "\n",
      "Top features por importancia (permutación):\n",
      "EC2-Instances($)                  0.926207\n",
      "EC2-Other($)                      0.737711\n",
      "Elastic File System($)            0.546623\n",
      "Backup($)                         0.503320\n",
      "DataSync($)                       0.481063\n",
      "S3($)                             0.466884\n",
      "Total costs($)                    0.447476\n",
      "Relational Database Service($)    0.422563\n",
      "CloudWatch($)                     0.405069\n",
      "Key Management Service($)         0.357044\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ---------- 8. predecir y marcar anomalías ----------\n",
    "test_scores = best_model.decision_function(X_test)   # mayor => más normal\n",
    "test_pred = best_model.predict(X_test)                # 1 normal, -1 anomalía\n",
    "test_anomaly = np.where(test_pred == 1, 0, 1)         # 1 = anomalía (más intuitivo)\n",
    "\n",
    "test_out = test_df.copy()\n",
    "test_out['anomaly_score'] = test_scores\n",
    "test_out['anomaly'] = test_anomaly\n",
    "\n",
    "print(\"\\nAnomalías en test (conteo):\", int(test_out['anomaly'].sum()))\n",
    "print(test_out[['anomaly_score','anomaly']].head(10))\n",
    "\n",
    "# ---------- 9. importancia de features (permutación) ----------\n",
    "# Usamos como 'y' las scores del modelo en train y medimos R2 del estimator.decision_function\n",
    "y_train_scores = best_model.decision_function(X_train)\n",
    "\n",
    "def scoring_fn_r2(estimator, X, y):\n",
    "    return r2_score(y, estimator.decision_function(X))\n",
    "\n",
    "perm = permutation_importance(best_model, X_train, y_train_scores, scoring=scoring_fn_r2,\n",
    "                              n_repeats=PERM_REPEATS, random_state=42, n_jobs=-1)\n",
    "perm_importances = pd.Series(perm.importances_mean, index=cols).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop features por importancia (permutación):\")\n",
    "print(perm_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46991582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados guardados: test_anomaly_results.csv, feature_importances_permutation.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- 10. exportar resultados ----------\n",
    "test_out.to_csv(\"test_anomaly_results.csv\")\n",
    "perm_importances.to_csv(\"feature_importances_permutation.csv\")\n",
    "print(\"\\nResultados guardados: test_anomaly_results.csv, feature_importances_permutation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a91f37",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'modelo_entrenado_1.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Guardar el modelo entrenado\u001b[39;00m\n\u001b[32m      3\u001b[39m joblib.dump(iso, \u001b[33m'\u001b[39m\u001b[33mmodelo_entrenado_amico_1.pkl\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m modelo_cargado = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodelo_entrenado_1.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m pred = modelo_cargado.predict(X_test[:\u001b[32m5\u001b[39m])\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repositorio_UDEM/MLops_AMICO/.venv/lib/python3.12/site-packages/joblib/numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'modelo_entrenado_1.pkl'"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Guardar el modelo entrenado\n",
    "joblib.dump(iso, 'modelo_entrenado_amico_1.pkl')\n",
    "modelo_cargado = joblib.load('modelo_entrenado_1.pkl')\n",
    "pred = modelo_cargado.predict(X_test[:5])\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLops_AMICO (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
