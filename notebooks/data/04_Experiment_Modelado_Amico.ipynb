{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c8d50e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/angeleduardogamarrarios/Repositorio_UDEM/MLops_AMICO/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Carga de librerias\n",
    "# ------------------\n",
    "# Librerias de uso general\n",
    "import holidays\n",
    "from google.cloud import storage\n",
    "\n",
    "# Manejo de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM  # Importación para OC-SVM\n",
    "from sklearn.neighbors import LocalOutlierFactor # Importación para LOF\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Estadística y series temporales\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Se importan las funciones\n",
    "from sklearn.metrics import  mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a1a5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 0. configuración ----------\n",
    "CSV_PATH = \"/Users/angeleduardogamarrarios/Repositorio_UDEM/MLops_AMICO/data/costs.csv\"       # ajusta si hace falta\n",
    "TEST_DAYS = 60               # últimos N días para test\n",
    "RND_ITER = 12                # RandomizedSearchCV iteraciones (ajusta)\n",
    "TS_SPLITS = 3                # TimeSeriesSplit folds (ajusta)\n",
    "PERM_REPEATS = 10            # permutación (ajusta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2545613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filas: 351 Columnas: ['Relational Database Service($)', 'EC2-Instances($)', 'FSx($)', 'Elastic File System($)', 'EC2-Other($)', 'CloudWatch($)', 'S3($)', 'Elastic Load Balancing($)', 'Backup($)', 'Key Management Service($)', 'DataSync($)', 'Secrets Manager($)', 'Resilience Hub($)', 'Total costs($)']\n"
     ]
    }
   ],
   "source": [
    "# ---------- 1. carga y limpieza básica ----------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# eliminar fila de \"Service total\" si existe\n",
    "df = df[df['Service'] != 'Service total']\n",
    "\n",
    "# convertir columna de fecha y orden\n",
    "df['Service'] = pd.to_datetime(df['Service'])\n",
    "df = df.rename(columns={'Service':'date'}).sort_values('date').set_index('date')\n",
    "\n",
    "# forzar numérico y revisar columnas\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "num_cols = df.columns.tolist()\n",
    "\n",
    "print(\"Filas:\", len(df), \"Columnas:\", num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22d9e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen estadístico:\n",
      "                                count        mean         std           min  \\\n",
      "Relational Database Service($)  351.0   97.240692   87.975006  1.491792e+01   \n",
      "EC2-Instances($)                329.0   71.525192   25.109641  5.154986e-01   \n",
      "FSx($)                          351.0   14.678272    1.855596  1.208052e+01   \n",
      "Elastic File System($)          351.0    8.064240    6.456387  1.786584e+00   \n",
      "EC2-Other($)                    351.0    6.076750    2.900253  3.117944e-01   \n",
      "CloudWatch($)                   351.0    4.398582    7.681060  3.109096e-03   \n",
      "S3($)                           351.0    2.218248    0.787167  1.023371e+00   \n",
      "Elastic Load Balancing($)       351.0    2.162289    0.004710  2.160000e+00   \n",
      "Backup($)                       350.0    1.514773    1.210465  2.810710e-01   \n",
      "Key Management Service($)       351.0    0.260617    0.032317  2.263914e-01   \n",
      "DataSync($)                     119.0    0.286251    1.415347  6.440000e-08   \n",
      "Secrets Manager($)              349.0    0.000513    0.000527  2.000000e-05   \n",
      "Resilience Hub($)                 3.0    0.000000    0.000000  0.000000e+00   \n",
      "Total costs($)                  351.0  203.749835  123.021125  3.366081e+01   \n",
      "\n",
      "                                       25%         50%         75%         max  \n",
      "Relational Database Service($)   41.109960   66.762613  118.507451  455.461320  \n",
      "EC2-Instances($)                 60.758766   68.402379   95.067461  100.737059  \n",
      "FSx($)                           12.584705   15.468255   16.056011   18.491514  \n",
      "Elastic File System($)            2.040983    4.627442   14.108374   20.584791  \n",
      "EC2-Other($)                      5.541606    6.124120    7.267743   11.823171  \n",
      "CloudWatch($)                     0.478766    0.682977    1.661212   25.383237  \n",
      "S3($)                             1.693569    2.084439    3.082417    3.540238  \n",
      "Elastic Load Balancing($)         2.160033    2.160623    2.162270    2.194784  \n",
      "Backup($)                         0.322099    1.043619    2.500407    4.093945  \n",
      "Key Management Service($)         0.237089    0.258728    0.267448    0.356186  \n",
      "DataSync($)                       0.000001    0.000001    0.001859    8.782443  \n",
      "Secrets Manager($)                0.000245    0.000455    0.000640    0.006030  \n",
      "Resilience Hub($)                 0.000000    0.000000    0.000000    0.000000  \n",
      "Total costs($)                  127.055868  169.308792  262.386479  606.597727  \n",
      "\n",
      "Skewness (top):\n",
      "Secrets Manager($)                6.825494\n",
      "DataSync($)                       5.243936\n",
      "Elastic Load Balancing($)         3.825244\n",
      "Relational Database Service($)    1.866276\n",
      "CloudWatch($)                     1.677079\n",
      "Key Management Service($)         1.537350\n",
      "Total costs($)                    0.975280\n",
      "Elastic File System($)            0.664392\n",
      "Backup($)                         0.622538\n",
      "S3($)                             0.119986\n",
      "dtype: float64\n",
      "\n",
      "Media por día de la semana (muestra):\n",
      "          Relational Database Service($)  EC2-Instances($)     FSx($)  \\\n",
      "date                                                                    \n",
      "Friday                        104.490301         76.534417  14.801883   \n",
      "Monday                         97.532270         75.251444  14.722385   \n",
      "Saturday                       90.899097         44.656602  14.535091   \n",
      "Sunday                         83.598953         75.433716  14.472918   \n",
      "Thursday                      103.113998         76.944815  14.770521   \n",
      "\n",
      "          Elastic File System($)  EC2-Other($)  CloudWatch($)     S3($)  \\\n",
      "date                                                                      \n",
      "Friday                  8.218541      7.052756       4.532564  2.239331   \n",
      "Monday                  7.949294      6.826152       4.525775  2.209111   \n",
      "Saturday                8.179670      4.083944       3.875943  2.207464   \n",
      "Sunday                  7.941768      3.811028       3.811151  2.189945   \n",
      "Thursday                8.026860      6.897890       4.715092  2.233260   \n",
      "\n",
      "          Elastic Load Balancing($)  Backup($)  Key Management Service($)  \\\n",
      "date                                                                        \n",
      "Friday                     2.163086   1.543027                   0.261761   \n",
      "Monday                     2.162876   1.479395                   0.260807   \n",
      "Saturday                   2.161187   1.527428                   0.260784   \n",
      "Sunday                     2.160560   1.518090                   0.258952   \n",
      "Thursday                   2.163299   1.514082                   0.260705   \n",
      "\n",
      "          DataSync($)  Secrets Manager($)  Resilience Hub($)  Total costs($)  \n",
      "date                                                                          \n",
      "Friday       1.264603            0.000579                NaN      222.318796  \n",
      "Monday       0.073895            0.000552                0.0      212.945186  \n",
      "Saturday     0.053987            0.000396                0.0      172.377762  \n",
      "Sunday       0.000705            0.000420                NaN      162.006882  \n",
      "Thursday     0.001682            0.000621                0.0      220.641680  \n"
     ]
    }
   ],
   "source": [
    "# ---------- 2. EDA rápido (resumen + skew + patrón semanal) ----------\n",
    "print(\"\\nResumen estadístico:\")\n",
    "print(df.describe().T)\n",
    "\n",
    "skewness = df.skew().sort_values(ascending=False)\n",
    "print(\"\\nSkewness (top):\")\n",
    "print(skewness.head(10))\n",
    "\n",
    "weekly_mean = df.groupby(df.index.day_name()).mean()\n",
    "print(\"\\nMedia por día de la semana (muestra):\")\n",
    "print(weekly_mean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97db7068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3. imputación ----------\n",
    "# Strategy: cambiar a 0\n",
    "df_imputed = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80fcfa13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box-Cox aplicado a Secrets Manager($), lambda=0.3130\n",
      "Box-Cox aplicado a DataSync($), lambda=-0.7391\n",
      "Box-Cox aplicado a Elastic Load Balancing($), lambda=-693.3143\n",
      "Box-Cox aplicado a Relational Database Service($), lambda=-0.0614\n",
      "Box-Cox aplicado a CloudWatch($), lambda=0.0712\n",
      "Box-Cox aplicado a Key Management Service($), lambda=-4.2383\n"
     ]
    }
   ],
   "source": [
    "# ---------- 4. Box-Cox selectivo ----------\n",
    "# Aplicar Box-Cox solo a columnas muy sesgadas (skew > 1)\n",
    "skewed_cols = skewness[skewness > 1].index.tolist()\n",
    "df_bc = df_imputed.copy()\n",
    "\n",
    "for c in skewed_cols:\n",
    "    # Box-Cox exige valores > 0. si hay ceros o negativos shift pequeño\n",
    "    min_val = df_bc[c].min()\n",
    "    shift = 0.0 if min_val > 0 else abs(min_val) + 1e-6\n",
    "    try:\n",
    "        transformed, lam = boxcox(df_bc[c] + shift)\n",
    "        df_bc[c] = transformed\n",
    "        print(f\"Box-Cox aplicado a {c}, lambda={lam:.4f}\")\n",
    "    except Exception as e:\n",
    "        # fallback log1p si falla\n",
    "        df_bc[c] = np.log1p(df_bc[c] + shift)\n",
    "        print(f\"Box-Cox falló en {c}, aplicado log1p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3da2cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 5. normalización por día de la semana ----------\n",
    "df_bc['day_of_week'] = df_bc.index.day_name()\n",
    "scaled = df_bc.copy()\n",
    "features = [c for c in num_cols]  # lista de features reales\n",
    "\n",
    "for day in scaled['day_of_week'].unique():\n",
    "    mask = scaled['day_of_week'] == day\n",
    "    if mask.sum() < 2:\n",
    "        # si no hay suficientes ejemplos para el día, omitir\n",
    "        continue\n",
    "    scaler = StandardScaler()\n",
    "    scaled.loc[mask, features] = scaler.fit_transform(scaled.loc[mask, features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a04092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train shape: (290, 14), Test shape: (61, 14)\n"
     ]
    }
   ],
   "source": [
    "# ---------- 6. división train/test temporal ----------\n",
    "split_date = scaled.index.max() - pd.Timedelta(days=TEST_DAYS)\n",
    "train_df = scaled[scaled.index < split_date].drop(columns=['day_of_week'])\n",
    "test_df = scaled[scaled.index >= split_date].drop(columns=['day_of_week'])\n",
    "\n",
    "X_train = train_df.values\n",
    "X_test = test_df.values\n",
    "cols = train_df.columns.tolist()\n",
    "\n",
    "print(f\"\\nTrain shape: {X_train.shape}, Test shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b53947e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "\n",
      "Mejores parámetros (RandomizedSearch):\n",
      "{'n_estimators': 50, 'max_samples': 0.9, 'max_features': 0.7, 'contamination': 0.001}\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "\n",
      "Mejores parámetros (GridSearch):\n",
      "{'contamination': 0.0005, 'max_features': 0.7, 'max_samples': 0.9, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "# ---------- 7. búsqueda de hiperparámetros ----------\n",
    "# scoring personalizado: media de decision_function (cuanto mayor, mejor)\n",
    "def scoring_fn(estimator, X, y=None):\n",
    "    return float(np.mean(estimator.decision_function(X)))\n",
    "\n",
    "iso = IsolationForest(random_state=42)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_samples': [0.5, 0.7, 0.9, 'auto'],\n",
    "    'contamination': [0.001, 0.005, 0.01, 0.02, 0.05],\n",
    "    'max_features': [0.5, 0.7, 1.0]\n",
    "}\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=TS_SPLITS)\n",
    "rnd = RandomizedSearchCV(iso, param_distributions=param_dist, n_iter=RND_ITER,\n",
    "                         cv=tscv, random_state=42, n_jobs=-1, scoring=scoring_fn, verbose=1)\n",
    "rnd.fit(X_train)\n",
    "\n",
    "print(\"\\nMejores parámetros (RandomizedSearch):\")\n",
    "print(rnd.best_params_)\n",
    "\n",
    "# ajustar pequeño grid alrededor del mejor para refinar (GridSearch)\n",
    "best = rnd.best_params_\n",
    "grid = {\n",
    "    'n_estimators': sorted(list({max(10, best['n_estimators']-50), best['n_estimators'], best['n_estimators']+50})),\n",
    "    'max_samples': sorted(list(set([best['max_samples'] if best['max_samples']=='auto' else max(0.1, best['max_samples']-0.1), best['max_samples'], min(1.0, best['max_samples']+0.1)]))),\n",
    "    'contamination': sorted(list({max(0.0005, best['contamination']/2), best['contamination'], min(0.1, best['contamination']*2)})),\n",
    "    'max_features': sorted(list({max(0.1, best['max_features']-0.2), best['max_features'], min(1.0, best['max_features']+0.2)}))\n",
    "}\n",
    "# limpiar valores inválidos\n",
    "grid['max_samples'] = [v for v in grid['max_samples'] if (isinstance(v, str) or (isinstance(v, float) and 0 < v <= 1))]\n",
    "\n",
    "gsearch = GridSearchCV(IsolationForest(random_state=42), param_grid=grid, cv=tscv, n_jobs=-1, scoring=scoring_fn, verbose=1)\n",
    "gsearch.fit(X_train)\n",
    "\n",
    "print(\"\\nMejores parámetros (GridSearch):\")\n",
    "print(gsearch.best_params_)\n",
    "\n",
    "best_model = gsearch.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ce20e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anomalías en test (conteo): 27\n",
      "            anomaly_score  anomaly\n",
      "date                              \n",
      "2025-03-18       0.112032        0\n",
      "2025-03-19       0.124936        0\n",
      "2025-03-20       0.112604        0\n",
      "2025-03-21       0.112991        0\n",
      "2025-03-22       0.107334        0\n",
      "2025-03-23       0.075393        0\n",
      "2025-03-24       0.070000        0\n",
      "2025-03-25       0.049060        0\n",
      "2025-03-26       0.062226        0\n",
      "2025-03-27       0.062226        0\n",
      "\n",
      "Top features por importancia (permutación):\n",
      "EC2-Instances($)                  0.926207\n",
      "EC2-Other($)                      0.737711\n",
      "Elastic File System($)            0.546623\n",
      "Backup($)                         0.503320\n",
      "DataSync($)                       0.481063\n",
      "S3($)                             0.466884\n",
      "Total costs($)                    0.447476\n",
      "Relational Database Service($)    0.422563\n",
      "CloudWatch($)                     0.405069\n",
      "Key Management Service($)         0.357044\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# ---------- 8. predecir y marcar anomalías ----------\n",
    "test_scores = best_model.decision_function(X_test)   # mayor => más normal\n",
    "test_pred = best_model.predict(X_test)                # 1 normal, -1 anomalía\n",
    "test_anomaly = np.where(test_pred == 1, 0, 1)         # 1 = anomalía (más intuitivo)\n",
    "\n",
    "test_out = test_df.copy()\n",
    "test_out['anomaly_score'] = test_scores\n",
    "test_out['anomaly'] = test_anomaly\n",
    "\n",
    "print(\"\\nAnomalías en test (conteo):\", int(test_out['anomaly'].sum()))\n",
    "print(test_out[['anomaly_score','anomaly']].head(10))\n",
    "\n",
    "# ---------- 9. importancia de features (permutación) ----------\n",
    "# Usamos como 'y' las scores del modelo en train y medimos R2 del estimator.decision_function\n",
    "y_train_scores = best_model.decision_function(X_train)\n",
    "\n",
    "def scoring_fn_r2(estimator, X, y):\n",
    "    return r2_score(y, estimator.decision_function(X))\n",
    "\n",
    "perm = permutation_importance(best_model, X_train, y_train_scores, scoring=scoring_fn_r2,\n",
    "                              n_repeats=PERM_REPEATS, random_state=42, n_jobs=-1)\n",
    "perm_importances = pd.Series(perm.importances_mean, index=cols).sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop features por importancia (permutación):\")\n",
    "print(perm_importances.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46991582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados guardados: test_anomaly_results.csv, feature_importances_permutation.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------- 10. exportar resultados ----------\n",
    "test_out.to_csv(\"test_anomaly_results.csv\")\n",
    "perm_importances.to_csv(\"feature_importances_permutation.csv\")\n",
    "print(\"\\nResultados guardados: test_anomaly_results.csv, feature_importances_permutation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLops_AMICO (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
