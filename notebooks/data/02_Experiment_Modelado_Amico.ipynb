{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95de1e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de librerias\n",
    "# ------------------\n",
    "# Librerias de uso general\n",
    "import holidays\n",
    "from google.cloud import storage\n",
    "\n",
    "# Manejo de datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# Visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Estadística y series temporales\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Se importan las funciones\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4ea3a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapper = {\n",
    "    'Service': 'fecha',\n",
    "    'Relational Database Service($)': 'rds',\n",
    "    'EC2-Instances($)': 'ec2',\n",
    "    'FSx($)': 'fsx',\n",
    "    'Elastic File System($)': 'efs',\n",
    "    'EC2-Other($)': 'ec2_other',\n",
    "    'CloudWatch($)': 'cloudwatch',\n",
    "    'Elastic Load Balancing($)': 'elb',\n",
    "    'S3($)': 's3',\n",
    "    'Backup($)': 'backup',\n",
    "    'Key Management Service($)': 'kms',\n",
    "    'DataSync($)': 'data_sync',\n",
    "    'Secrets Manager($)': 'secrets_manager',\n",
    "    'Resilience Hub($)': 'resiliency',\n",
    "    'Total costs($)': 'total_costs'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e10de",
   "metadata": {},
   "source": [
    "Carga de los Datos\n",
    "Se hace la carga de los datos desde la ruta Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "783f1dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Service</th>\n",
       "      <th>Relational Database Service($)</th>\n",
       "      <th>EC2-Instances($)</th>\n",
       "      <th>FSx($)</th>\n",
       "      <th>Elastic File System($)</th>\n",
       "      <th>EC2-Other($)</th>\n",
       "      <th>CloudWatch($)</th>\n",
       "      <th>S3($)</th>\n",
       "      <th>Elastic Load Balancing($)</th>\n",
       "      <th>Backup($)</th>\n",
       "      <th>Key Management Service($)</th>\n",
       "      <th>DataSync($)</th>\n",
       "      <th>Secrets Manager($)</th>\n",
       "      <th>Resilience Hub($)</th>\n",
       "      <th>Total costs($)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Service total</td>\n",
       "      <td>34131.482733</td>\n",
       "      <td>23531.788153</td>\n",
       "      <td>5152.073356</td>\n",
       "      <td>2830.548352</td>\n",
       "      <td>2132.939335</td>\n",
       "      <td>1543.902136</td>\n",
       "      <td>778.604880</td>\n",
       "      <td>758.963353</td>\n",
       "      <td>530.170434</td>\n",
       "      <td>91.476443</td>\n",
       "      <td>34.06381</td>\n",
       "      <td>0.178930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71516.191916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-01</td>\n",
       "      <td>31.851525</td>\n",
       "      <td>4.612654</td>\n",
       "      <td>12.472440</td>\n",
       "      <td>1.789663</td>\n",
       "      <td>0.611443</td>\n",
       "      <td>0.036701</td>\n",
       "      <td>1.156349</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>0.281071</td>\n",
       "      <td>0.234161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.0</td>\n",
       "      <td>55.206262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-06-02</td>\n",
       "      <td>61.039787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.460864</td>\n",
       "      <td>1.789663</td>\n",
       "      <td>0.322181</td>\n",
       "      <td>0.016653</td>\n",
       "      <td>1.126354</td>\n",
       "      <td>2.160000</td>\n",
       "      <td>0.281071</td>\n",
       "      <td>0.235058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.431926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-06-03</td>\n",
       "      <td>71.416393</td>\n",
       "      <td>63.161250</td>\n",
       "      <td>12.654853</td>\n",
       "      <td>1.790544</td>\n",
       "      <td>5.613880</td>\n",
       "      <td>0.429317</td>\n",
       "      <td>1.119325</td>\n",
       "      <td>2.160042</td>\n",
       "      <td>0.281071</td>\n",
       "      <td>0.240077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158.866978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-06-04</td>\n",
       "      <td>54.833184</td>\n",
       "      <td>68.402379</td>\n",
       "      <td>12.658942</td>\n",
       "      <td>1.791199</td>\n",
       "      <td>5.682819</td>\n",
       "      <td>0.459109</td>\n",
       "      <td>1.116106</td>\n",
       "      <td>2.160139</td>\n",
       "      <td>0.281314</td>\n",
       "      <td>0.238019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000225</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.623435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Service  Relational Database Service($)  EC2-Instances($)  \\\n",
       "0  Service total                    34131.482733      23531.788153   \n",
       "1     2024-06-01                       31.851525          4.612654   \n",
       "2     2024-06-02                       61.039787               NaN   \n",
       "3     2024-06-03                       71.416393         63.161250   \n",
       "4     2024-06-04                       54.833184         68.402379   \n",
       "\n",
       "        FSx($)  Elastic File System($)  EC2-Other($)  CloudWatch($)  \\\n",
       "0  5152.073356             2830.548352   2132.939335    1543.902136   \n",
       "1    12.472440                1.789663      0.611443       0.036701   \n",
       "2    12.460864                1.789663      0.322181       0.016653   \n",
       "3    12.654853                1.790544      5.613880       0.429317   \n",
       "4    12.658942                1.791199      5.682819       0.459109   \n",
       "\n",
       "        S3($)  Elastic Load Balancing($)   Backup($)  \\\n",
       "0  778.604880                 758.963353  530.170434   \n",
       "1    1.156349                   2.160000    0.281071   \n",
       "2    1.126354                   2.160000    0.281071   \n",
       "3    1.119325                   2.160042    0.281071   \n",
       "4    1.116106                   2.160139    0.281314   \n",
       "\n",
       "   Key Management Service($)  DataSync($)  Secrets Manager($)  \\\n",
       "0                  91.476443     34.06381            0.178930   \n",
       "1                   0.234161          NaN            0.000255   \n",
       "2                   0.235058          NaN            0.000295   \n",
       "3                   0.240077          NaN            0.000225   \n",
       "4                   0.238019          NaN            0.000225   \n",
       "\n",
       "   Resilience Hub($)  Total costs($)  \n",
       "0                0.0    71516.191916  \n",
       "1                0.0       55.206262  \n",
       "2                NaN       79.431926  \n",
       "3                NaN      158.866978  \n",
       "4                NaN      147.623435  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Datos con corte a Abril usar: \"costs.csv\"\n",
    "\n",
    "df = pd.read_csv('/Users/angeleduardogamarrarios/Repositorio_UDEM/MLops_AMICO/data/costs.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33c425db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeo de columnas \n",
    "df.rename(columns=column_mapper, inplace=True)\n",
    "# Elimina la fila con índice 0 y ka columna total_cost\n",
    "df = df.drop(0, axis=0)\n",
    "df = df.drop('total_costs', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde6646",
   "metadata": {},
   "source": [
    "# Prepocesar datos\n",
    "\n",
    "Para nuestro caso de analisis vamos a aplicar una normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1f7ff710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separar las variables continuas de las demás\n",
    "df_continuas=df[[\"rds\",\n",
    "                       \"ec2\",\n",
    "                       \"fsx\",\n",
    "                       \"efs\",\n",
    "                       \"ec2_other\",\n",
    "                       \"cloudwatch\",\n",
    "                       \"s3\",\n",
    "                       \"elb\",\n",
    "                       \"backup\",\n",
    "                       \"kms\",\n",
    "                       \"data_sync\",\n",
    "                       \"secrets_manager\",\n",
    "                       \"resiliency\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d615f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EL conjunto de datos se divide en una proporción de 60% (Entrenamiento), 20% (Validación) y 20% (Prueba).\n",
    "X_train_val, X_test = train_test_split(df_continuas, test_size=0.2, random_state=42)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.25, random_state=42) # 0.25 * 0.80 = 0.20 (20% para Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e736020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de 'rds' en X_train original: 96.0687\n",
      "Media de 'rds' en X_train escalado: -0.0000\n",
      "--------------------------------------------------\n",
      "✅ Pipeline y datos escalados listos. El preprocesador se guardó en: ./data/\n"
     ]
    }
   ],
   "source": [
    "# Normalizar los datos \n",
    "preprocessor_pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "preprocessor_pipeline.fit(X_train)\n",
    "\n",
    "X_train_scaled = preprocessor_pipeline.transform(X_train)\n",
    "X_val_scaled = preprocessor_pipeline.transform(X_val)\n",
    "X_test_scaled = preprocessor_pipeline.transform(X_test)\n",
    "\n",
    "# 4. Verificación y Persistencia (MLOps)\n",
    "print(f\"Media de 'rds' en X_train original: {X_train['rds'].mean():.4f}\")\n",
    "print(f\"Media de 'rds' en X_train escalado: {X_train_scaled[:, X_train.columns.get_loc('rds')].mean():.4f}\") # La media debe ser cercana a 0\n",
    "\n",
    "# Guardar el Pipeline (¡Esto es clave para el MLOps!)\n",
    "# Debes guardar el objeto preprocessor_pipeline para usarlo en producción\n",
    "# cuando llegue un nuevo dato de vino.\n",
    "\n",
    "OUTPUT_DIR = \"./data/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"preprocessor_pipeline.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(preprocessor_pipeline, f)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"✅ Pipeline y datos escalados listos. El preprocesador se guardó en: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac022dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verificación del DataFrame escalado ---\n",
      "Tipo de X_train_scaled_df: <class 'pandas.core.frame.DataFrame'>\n",
      "Las primeras 5 filas de X_train_scaled_df:\n",
      "          rds       ec2       fsx       efs  ec2_other  cloudwatch        s3  \\\n",
      "175 -0.055722 -0.455170  0.662202 -0.666575   0.129742   -0.405917 -0.247247   \n",
      "233 -0.069585  1.030785  0.492330  0.707702   0.541876   -0.471159  0.564653   \n",
      "351  3.891164 -0.703529  0.508479  1.861979  -1.265586   -0.166167  1.434175   \n",
      "75  -0.648316 -0.494546  0.333014 -0.940589   0.849970   -0.330212 -0.945479   \n",
      "66  -0.665479 -0.702253 -1.098427 -0.944476  -0.040083   -0.472826 -1.103862   \n",
      "\n",
      "          elb    backup       kms  data_sync  secrets_manager  resiliency  \n",
      "175 -0.408174 -0.575719  0.224661  -0.111943        -0.539126         0.0  \n",
      "233 -0.459486  0.489771 -0.058055  -0.111944        -0.140724         0.0  \n",
      "351 -0.116118 -0.381946  2.932068  -0.111943        -0.695362         0.0  \n",
      "75  -0.328749 -0.995102 -1.013494  -0.111943         0.062383         0.0  \n",
      "66  -0.393010 -0.998443 -0.983144  -0.111943        -0.437572         0.0  \n",
      "Media de 'rds' en X_train escalado (DF): -0.0000\n"
     ]
    }
   ],
   "source": [
    "# Crear los DF de nuevo con los datos ecaldos entregado en un array\n",
    "# 1. Crear el DataFrame escalado\n",
    "X_train_scaled_df = pd.DataFrame(\n",
    "    # 1. Los datos son el array de NumPy\n",
    "    X_train_scaled, \n",
    "    \n",
    "    # 2. Los índices (filas) se toman del DataFrame original\n",
    "    index=X_train.index, \n",
    "    \n",
    "    # 3. Los nombres de las columnas se toman del DataFrame original\n",
    "    columns=X_train.columns \n",
    ")\n",
    "\n",
    "# 2. Repetir para los otros conjuntos\n",
    "X_val_scaled_df = pd.DataFrame(\n",
    "    X_val_scaled, \n",
    "    index=X_val.index, \n",
    "    columns=X_val.columns\n",
    ")\n",
    "\n",
    "X_test_scaled_df = pd.DataFrame(\n",
    "    X_test_scaled, \n",
    "    index=X_test.index, \n",
    "    columns=X_test.columns\n",
    ")\n",
    "\n",
    "# 4. Verificación\n",
    "print(\"\\n--- Verificación del DataFrame escalado ---\")\n",
    "print(f\"Tipo de X_train_scaled_df: {type(X_train_scaled_df)}\")\n",
    "print(f\"Las primeras 5 filas de X_train_scaled_df:\\n{X_train_scaled_df.head()}\")\n",
    "print(f\"Media de 'rds' en X_train escalado (DF): {X_train_scaled_df['rds'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39343c67",
   "metadata": {},
   "source": [
    "Isolation Forest (El Enfoque Robusto)\n",
    "El Isolation Forest es rápido, escalable y muy robusto para datos multivariados con muchos outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1653278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Anomalías detectadas por Isolation Forest (ejemplo) ---\n",
      "Total de anomalías detectadas: 3\n",
      "          rds       ec2       fsx       efs  ec2_other  cloudwatch        s3  \\\n",
      "351  3.891164 -0.703529  0.508479  1.861979  -1.265586   -0.166167  1.434175   \n",
      "349  3.902792  0.535344  0.772158  1.861837   0.704645    1.506159  1.424562   \n",
      "344  3.171395 -0.539961  0.593477  1.861602  -0.722283   -0.178437  1.426032   \n",
      "\n",
      "          elb    backup       kms  data_sync  secrets_manager  resiliency  \\\n",
      "351 -0.116118 -0.381946  2.932068  -0.111943        -0.695362         0.0   \n",
      "349  6.726628  2.150503  2.939841  -0.111943         8.569433         0.0   \n",
      "344  3.981615  2.150536  2.931883  -0.111943        -0.539126         0.0   \n",
      "\n",
      "     is_anomaly_true  anomaly_iso_forest  score_iso_forest  \n",
      "351              1.0                  -1         -0.044952  \n",
      "349              1.0                  -1         -0.060478  \n",
      "344              1.0                  -1         -0.005552  \n",
      "F1-Score (enfocado en anomalías): 0.0000\n",
      "RMSE (entre puntuación ideal y decisión): 0.7920\n",
      "\n",
      "--- RESUMEN DE DESEMPEÑO ---\n",
      "F1_Score: 0.0000\n",
      "RMSE_Score: 0.7920\n"
     ]
    }
   ],
   "source": [
    "# Creamos una copia del DF de train original para poder ser usado en otros modelos \n",
    "X_train_scaled_df_is = X_train_scaled_df.copy() \n",
    "# creamos una columna de etiquetas verdaderas *para el ejemplo*\n",
    "# En la vida real, necesitarías cargar estas etiquetas.\n",
    "# Ejemplo de etiquetas verdaderas simuladas:\n",
    "# 99% normales (1), 1% anómalas (-1)\n",
    "X_train_size = X_train_scaled_df_is.shape[0]\n",
    "n_anomalies = int(X_train_size * 0.01)\n",
    "y_true = np.ones(X_train_size)\n",
    "y_true[:n_anomalies] = -1 # Los primeros datos son anómalos\n",
    "np.random.shuffle(y_true) # Mezclamos las etiquetas\n",
    "X_train_scaled_df_is['is_anomaly_true'] = y_true # Agregamos la columna de etiquetas verdaderas\n",
    "\n",
    "\n",
    "# Inicializar y entrenar el modelo\n",
    "iso_forest = IsolationForest(\n",
    "    n_estimators=100,\n",
    "    contamination=contamination_factor,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "X_train_features = X_train_scaled_df_is.copy() \n",
    "\n",
    "# Entrenar el modelo (No supervisado: no necesita etiquetas)\n",
    "iso_forest.fit(X_train_features.drop(columns=['is_anomaly_true'])) # Entrenamos sin la etiqueta verdadera\n",
    "\n",
    "# Predecir las anomalías: -1 para anomalía, 1 para dato normal\n",
    "X_train_scaled_df_is['anomaly_iso_forest'] = iso_forest.predict(X_train_features.drop(columns=['is_anomaly_true']))\n",
    "\n",
    "# Obtener la 'Puntuación de Anomalía'\n",
    "X_train_scaled_df_is['score_iso_forest'] = iso_forest.decision_function(X_train_features.drop(columns=['is_anomaly_true']))\n",
    "\n",
    "# Mostrar los días anómalos\n",
    "print(\"\\n--- Anomalías detectadas por Isolation Forest (ejemplo) ---\")\n",
    "anomalies_if = X_train_scaled_df_is[X_train_scaled_df_is['anomaly_iso_forest'] == -1]\n",
    "print(f\"Total de anomalías detectadas: {len(anomalies_if)}\")\n",
    "print(anomalies_if.head())\n",
    "# Nota: El umbral para la detección es ajustado por el 'contamination' que elegiste.\n",
    "\n",
    "# --- 2. CÁLCULO DE MÉTRICAS DE DESEMPEÑO ---\n",
    "\n",
    "# a) Métricas de Clasificación (F1-Score)\n",
    "# Necesitan las etiquetas verdaderas (y_true) y las etiquetas predichas (y_pred)\n",
    "\n",
    "y_pred = X_train_scaled_df_is['anomaly_iso_forest'].values\n",
    "y_true = X_train_scaled_df_is['is_anomaly_true'].values\n",
    "\n",
    "# El F1-Score es la media armónica entre precisión y exhaustividad.\n",
    "# Es ideal cuando las clases están desbalanceadas (muchos normales, pocos anómalos).\n",
    "f1 = f1_score(y_true, y_pred, average='binary', pos_label=-1) \n",
    "\n",
    "print(f\"F1-Score (enfocado en anomalías): {f1:.4f}\")\n",
    "\n",
    "# b) Métrica de Error (RMSE)\n",
    "# El RMSE se usa típicamente para modelos de Regresión, pero se puede usar para medir\n",
    "# qué tan lejos están las *puntuaciones* de anomalía de un valor objetivo.\n",
    "\n",
    "# **ADVERTENCIA sobre RMSE en Detección de Anomalías:**\n",
    "# El RMSE no es una métrica estándar para evaluar el rendimiento de clasificación\n",
    "# de Isolation Forest. No obstante, si se quiere forzar la métrica:\n",
    "\n",
    "# Creamos una 'puntuación ideal' donde los valores normales tienen 1.0 y los anómalos tienen un valor bajo (ej. 0.0)\n",
    "# Para el ejemplo, usaremos la puntuación de decisión (score_iso_forest)\n",
    "# La forma correcta de usar RMSE aquí es compleja, así que mostraremos cómo calcularlo\n",
    "# si tu objetivo fuese predecir la puntuación verdadera.\n",
    "\n",
    "# Para simplificar y mostrar el cálculo:\n",
    "# Asumamos que tu objetivo (y_true) son las etiquetas numéricas, y queremos ver\n",
    "# el \"error\" de la puntuación de decisión.\n",
    "# (Esto no es un uso estándar, pero cumple con el requisito de mostrar el cálculo)\n",
    "# Vamos a simular un 'score' objetivo: 1 para normal, 0 para anomalía\n",
    "y_target_score = np.where(y_true == 1, 1.0, 0.0) \n",
    "y_pred_score = X_train_scaled_df_is['score_iso_forest'].values # Las puntuaciones de decisión\n",
    "\n",
    "rmse_score = sqrt(mean_squared_error(y_target_score, y_pred_score))\n",
    "\n",
    "print(f\"RMSE (entre puntuación ideal y decisión): {rmse_score:.4f}\")\n",
    "\n",
    "\n",
    "# --- 3. RESUMEN DE MÉTRICAS ---\n",
    "\n",
    "# Puedes agrupar las métricas en un diccionario para un informe fácil\n",
    "performance_metrics = {\n",
    "    'F1_Score': f1,\n",
    "    'RMSE_Score': rmse_score,\n",
    "    # Otras métricas comunes en anomalías:\n",
    "    # 'Precision': precision_score(y_true, y_pred, pos_label=-1),\n",
    "    # 'Recall': recall_score(y_true, y_pred, pos_label=-1)\n",
    "}\n",
    "\n",
    "print(\"\\n--- RESUMEN DE DESEMPEÑO ---\")\n",
    "for metric, value in performance_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28a417",
   "metadata": {},
   "source": [
    "One-Class SVM (OC-SVM)\n",
    "OC-SVM intenta encontrar un hiperplano que separe la mayoría de los datos \"normales\" del origen, marcando los puntos que quedan fuera como anomalías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d6241a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Anomalías detectadas por One-Class SVM ---\n",
      "Total de anomalías detectadas: 13\n",
      "          rds       ec2       fsx       efs  ec2_other  cloudwatch        s3  \\\n",
      "351  3.891164 -0.703529  0.508479  1.861979  -1.265586   -0.166167  1.434175   \n",
      "262  0.139671  1.033179  1.427751  1.065621   0.089126   -0.418847  1.178391   \n",
      "323  1.836257  0.893155  1.078494  1.936324   1.628101    2.352258  1.486355   \n",
      "319  1.327360  1.053907  1.066812  1.783952   1.587446    2.344113  1.592530   \n",
      "39   0.091915  0.316006 -1.169444 -0.958107   0.282216   -0.454032 -1.393626   \n",
      "\n",
      "          elb    backup       kms  data_sync  secrets_manager  resiliency  \\\n",
      "351 -0.116118 -0.381946  2.932068  -0.111943        -0.695362         0.0   \n",
      "262 -0.219831  0.808493  0.825572  -0.107682         2.460605         0.0   \n",
      "323 -0.457923  1.644362  2.269787  -0.111943        -0.226654         0.0   \n",
      "319  0.374527  1.466923  2.277282  -0.111943         2.093451         0.0   \n",
      "39   0.096049 -1.003566 -0.909490  -0.111943         0.374855         0.0   \n",
      "\n",
      "     is_anomaly_true  anomaly_iso_forest  score_iso_forest  anomaly_oc_svm  \\\n",
      "351              1.0                  -1         -0.044396              -1   \n",
      "262              1.0                   1          0.181706              -1   \n",
      "323              1.0                   1          0.185247              -1   \n",
      "319              1.0                   1          0.115836              -1   \n",
      "39               1.0                   1          0.158459              -1   \n",
      "\n",
      "     score_oc_svm  \n",
      "351     -0.000381  \n",
      "262     -0.000367  \n",
      "323     -0.000264  \n",
      "319     -0.000208  \n",
      "39      -0.000197  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import OneClassSVM  # Importación para OC-SVM\n",
    "from sklearn.neighbors import LocalOutlierFactor # Importación para LOF\n",
    "\n",
    "# Creamos una copia del DF de train original para poder ser usado en otros modelos \n",
    "X_train_scaled_df_oc = X_train_scaled_df.copy() \n",
    "\n",
    "# --- CONFIGURACIÓN DEL MODELO ---\n",
    "\n",
    "# El parámetro 'nu' es el equivalente al 'contamination' factor,\n",
    "# representando la fracción superior de anomalías esperadas.\n",
    "# 'gamma' define la influencia de un punto de entrenamiento individual\n",
    "# (usamos 'auto' para que se calibre por 1/n_features).\n",
    "contamination_factor = 0.01  # Esperamos un 1% de anomalías\n",
    "\n",
    "# Inicializar el modelo OC-SVM\n",
    "oc_svm = OneClassSVM(\n",
    "    kernel='rbf',  # Kernel de función de base radial, común en anomalías\n",
    "    gamma='auto',\n",
    "    nu=contamination_factor\n",
    ")\n",
    "\n",
    "X_train_features = X_train_scaled_df_is.copy() \n",
    "\n",
    "# Entrenar el modelo\n",
    "oc_svm.fit(X_train_features)\n",
    "\n",
    "# Predecir las anomalías: -1 para anomalía, 1 para dato normal\n",
    "X_train_scaled_df_oc['anomaly_oc_svm'] = oc_svm.predict(X_train_features)\n",
    "\n",
    "# Obtener la 'Puntuación de Anomalía' (cuanto menor, más anómalo)\n",
    "X_train_scaled_df_oc['score_oc_svm'] = oc_svm.decision_function(X_train_features)\n",
    "\n",
    "# Mostrar los días anómalos\n",
    "print(\"\\n--- Anomalías detectadas por One-Class SVM ---\")\n",
    "anomalies_ocsvm = X_train_scaled_df_oc[X_train_scaled_df_oc['anomaly_oc_svm'] == -1]\n",
    "print(f\"Total de anomalías detectadas: {len(anomalies_ocsvm)}\")\n",
    "print(anomalies_ocsvm.sort_values(by='score_oc_svm').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10addea3",
   "metadata": {},
   "source": [
    "Local Outlier Factor (LOF) LOF es un modelo basado en la densidad. Mide cuán aislado está un punto con respecto a sus vecinos más cercanos. Un score LOF mucho mayor que 1 indica una anomalía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9dd7acbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Anomalías detectadas por Local Outlier Factor (LOF) ---\n",
      "Total de anomalías detectadas: 3\n",
      "          rds       ec2       fsx       efs  ec2_other  cloudwatch        s3  \\\n",
      "203 -0.328424  0.436305  0.450120 -0.129187   0.464148   -0.462137 -0.085947   \n",
      "224 -0.250073  1.075655  0.459695  0.194370   0.590359   -0.474813  0.177261   \n",
      "252  0.036355  1.020134  1.813887  0.904283   0.227462   -0.380775  1.060813   \n",
      "\n",
      "          elb    backup       kms  data_sync  secrets_manager  resiliency  \\\n",
      "203  0.263859 -0.138673 -0.053058   9.541929        -0.671926         0.0   \n",
      "224 -0.045358  0.028645  0.061032  -0.111942         8.460067         0.0   \n",
      "252  0.256140  2.096955  0.797905  10.679210        -0.086041         0.0   \n",
      "\n",
      "     is_anomaly_true  anomaly_iso_forest  score_iso_forest  anomaly_lof  \\\n",
      "203              1.0                   1          0.132619           -1   \n",
      "224              1.0                   1          0.125949           -1   \n",
      "252              1.0                   1          0.059204           -1   \n",
      "\n",
      "     score_lof  \n",
      "203   6.818001  \n",
      "224   6.029630  \n",
      "252   5.321000  \n"
     ]
    }
   ],
   "source": [
    "# Creamos una copia del DF de train original para poder ser usado en otros modelos \n",
    "X_train_scaled_df_lof = X_train_scaled_df.copy() \n",
    "\n",
    "# --- CONFIGURACIÓN DEL MODELO ---\n",
    "\n",
    "# 'n_neighbors': Número de vecinos a considerar (típicamente entre 20 y 100).\n",
    "# 'contamination': La proporción esperada de anomalías (similar a Isolation Forest).\n",
    "n_neighbors = 20\n",
    "contamination_factor = 0.01\n",
    "\n",
    "# Inicializar el modelo LOF\n",
    "# Nota: LOF se inicializa en 'novelty=False' para fines de predicción.\n",
    "lof = LocalOutlierFactor(\n",
    "    n_neighbors=n_neighbors,\n",
    "    contamination=contamination_factor,\n",
    "    novelty=False  # Para usarlo en modo detección de outliers en el conjunto de entrenamiento\n",
    ")\n",
    "\n",
    "# Ajustar y predecir (LOF realiza el ajuste y la predicción en un solo paso)\n",
    "X_train_scaled_df_lof['anomaly_lof'] = lof.fit_predict(X_train_scaled_df_lof)\n",
    "\n",
    "# La 'Puntuación de Anomalía' de LOF es el factor LOF (cuanto mayor, más anómalo)\n",
    "# Nota: LOF devuelve el Factor de Anormalidad Local Negativo. Lo negamos para que\n",
    "# los valores más altos sigan significando mayor anomalía, como es habitual.\n",
    "X_train_scaled_df_lof['score_lof'] = -lof.negative_outlier_factor_\n",
    "\n",
    "# Mostrar los días anómalos\n",
    "print(\"\\n--- Anomalías detectadas por Local Outlier Factor (LOF) ---\")\n",
    "anomalies_lof = X_train_scaled_df_lof[X_train_scaled_df_lof['anomaly_lof'] == -1]\n",
    "print(f\"Total de anomalías detectadas: {len(anomalies_lof)}\")\n",
    "print(anomalies_lof.sort_values(by='score_lof', ascending=False).head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLops_AMICO (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
